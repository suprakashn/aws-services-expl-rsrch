Prerequisites:
 --> Install awscli and configure it with aws keys so that your system can interact with AWS.
 --> Install eksctl and kubectl in your system. Use these links for instructions (
   https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html
   https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html)

Step 1: Create EKS Cluster
  Use the sample template (create_EKS_cluster.yaml) to create the EKS cluster. The template creates the cluster 2 fargate profiles (associated with 2 kubernetes namespaces). Execute the following command from the CLI.
  
  --> eksctl create cluster -f create_EKS_cluster.yaml
  
NOTE: Since this cluster is based on Fargate, there would be no EC2 instance running after the creation of the cluster.


Step 2: Identity provider creation
  Create an IAM OIDC provider for your cluster if it is already not created. In order to test that you can execute the following commands from CLI.
  
  --> aws eks describe-cluster --name "<cluster-name>" --query "cluster.identity.oidc.issuer" --output text
      Output Example: https://oidc.eks.us-east-2.amazonaws.com/id/115D1C58230BE51D6DCCE7A879E94066
	 
  --> aws iam list-open-id-connect-providers | grep 115D1C58230BE51D6DCCE7A879E94066
      If there is any output then the identity provider is already created. If no result then create IAM OIDC provider
	 
  --> eksctl utils associate-iam-oidc-provider --cluster "<cluster-name>" --approve


Step 3: Allow access to EMR on EKS to fargate namespace
  This step is to provide EMR on EKS the access to the namespace created within the fargate profile. In this example there were couple of namespaces created in the cluster (default, fp-dev).
  
  --> eksctl create iamidentitymapping \
       --cluster <cluster-name> \
       --namespace <name-space> \
       --service-name "emr-containers"

	
Step 4: Establish trust between the namespace and IAM role
  The IAM role which would be used to execute the jobs would have to have the namespace as a trusted entity. For that you need to establish using the OIDC provider created earlier

  --> aws emr-containers update-role-trust-policy \
       --cluster-name <cluster-name> \
       --namespace <namespace> \
       --role-name <IAM-Role>
	

Step 5: Register EKS with EMR
  Create the virtual cluster associated to the namespace. Keep a note of the virtual-cluster-id. It would be used to submit any jobs in the future
  
  --> aws emr-containers create-virtual-cluster \
       --name <cluster-name> \
       --container-provider '{
         "id": "<cluster-name>",
         "type": "EKS",
         "info": {
           "eksInfo": {
            "namespace": "<namespace>"
           }
         }
       }'
	   

Step 6: Submit spark job
  Submitting the spark job (with dynamic resource allocation) in the cluster

  --> aws emr-containers start-job-run \
        --virtual-cluster-id=<virtual-cluster-id> \
        --name=<job-name> \
        --execution-role-arn=<IAM-Role> \
        --release-label=emr-6.3.0-latest \
        --job-driver='{
          "sparkSubmitJobDriver": {
            "entryPoint": "<script-name>",
            "sparkSubmitParameters": "<spark-conf-parameters>"
          }
        }'\
        --configuration-overrides='{
        	"applicationConfiguration": [
            {
              "classification": "spark-defaults", 
              "properties": {
                "spark.dynamicAllocation.enabled":"true",
                "spark.dynamicAllocation.shuffleTracking.enabled":"true",
                "spark.dynamicAllocation.minExecutors":"<min executors>",
                "spark.dynamicAllocation.maxExecutors":"<max executors>",
                "spark.dynamicAllocation.initialExecutors":"<init executors>",
                "spark.dynamicAllocation.schedulerBacklogTimeout": "1s",
                "spark.dynamicAllocation.executorIdleTimeout": "5s"
               }
            }
          ],
      	"monitoringConfiguration":
      	  {
      		"persistentAppUI": "ENABLED",
      		"s3MonitoringConfiguration": {
      			"logUri": "<s3-log-path>"
      		 }
      	  }
        }'
	
  Example:  
  --> aws emr-containers start-job-run \
        --virtual-cluster-id=jerdthnadi6lzionex9shyw71 \
        --name=2482-spark-on-eks-demo \
        --execution-role-arn=arn:aws:iam::076931226898:role/2482-misc-service-role \
        --release-label=emr-6.3.0-latest \
        --job-driver='{
          "sparkSubmitJobDriver": {
            "entryPoint": "s3://2482-bucket/code/sample.py",
            "sparkSubmitParameters": "--conf spark.executor.instances=1 --conf spark.executor.memory=1G --conf spark.executor.cores=1 --conf spark.driver.cores=1 --conf spark.kubernetes.container.image=076931226898.dkr.ecr.us-east-2.amazonaws.com/2482-spark-images"
          }
        }'\
        --configuration-overrides='{
        	"applicationConfiguration": [
            {
              "classification": "spark-defaults", 
              "properties": {
                "spark.dynamicAllocation.enabled":"true",
                "spark.dynamicAllocation.shuffleTracking.enabled":"true",
                "spark.dynamicAllocation.minExecutors":"1",
                "spark.dynamicAllocation.maxExecutors":"10",
                "spark.dynamicAllocation.initialExecutors":"1",
                "spark.dynamicAllocation.schedulerBacklogTimeout": "1s",
                "spark.dynamicAllocation.executorIdleTimeout": "5s"
               }
            }
          ],
      	"monitoringConfiguration":
      	  {
      		"persistentAppUI": "ENABLED",
      		"s3MonitoringConfiguration": {
      			"logUri": "s3://2482-bucket/logs/"
      		 }
      	  }
        }'
  

=========================================
How to submit spark jobs within docker containers:

Prerequisites:
  --> Install Docker: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html

  
Step 1: Pull the required base image
  To build the container, a base image has to be used. Use the link (https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/docker-custom-images-tag.html) to choose the base image based on the region.
  
    --> aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin 711395599931.dkr.ecr.us-east-2.amazonaws.com
	
	--> docker pull 711395599931.dkr.ecr.us-east-2.amazonaws.com/spark/emr-6.3.0:latest


Step 2: Build the container image 	
  Create a docker file which would be used to build the container image. Following command will create the final image using the base image and the docker file.
  
    --> docker build -t spark-demo-image .


Step 3: Register and tag repository in ECR
  --> aws ecr create-repository --repository-name <repo-name> --region us-east-2
  
  --> docker tag <tag-name> 076931226898.dkr.ecr.us-east-2.amazonaws.com/<repo-name>
  
  
Step 4: Push the docker image to ECR
  Login to the AWS ECR:
  
    --> aws ecr get-login-password | docker login --username AWS --password-stdin 076931226898.dkr.ecr.us-east-2.amazonaws.com
	
	--> docker push 076931226898.dkr.ecr.us-east-2.amazonaws.com/<repo-name>


Step 5: Submit spark job within docker image
  Submitting the spark job (with dynamic resource allocation) in the docker image

  --> aws emr-containers start-job-run \
        --virtual-cluster-id=<virtual-cluster-id> \
        --name=<job-name> \
        --execution-role-arn=<IAM-Role> \
        --release-label=emr-6.3.0-latest \
        --job-driver='{
          "sparkSubmitJobDriver": {
            "entryPoint": "<script-name>",
            "sparkSubmitParameters": "<spark-conf-parameters>
			--conf spark.kubernetes.container.image=076931226898.dkr.ecr.us-east-2.amazonaws.com/<repo-name>"
          }
        }'\
        --configuration-overrides='{
        	"applicationConfiguration": [
            {
              "classification": "spark-defaults", 
              "properties": {
                "spark.dynamicAllocation.enabled":"true",
                "spark.dynamicAllocation.shuffleTracking.enabled":"true",
                "spark.dynamicAllocation.minExecutors":"<min executors>",
                "spark.dynamicAllocation.maxExecutors":"<max executors>",
                "spark.dynamicAllocation.initialExecutors":"<init executors>",
                "spark.dynamicAllocation.schedulerBacklogTimeout": "1s",
                "spark.dynamicAllocation.executorIdleTimeout": "5s"
               }
            }
          ],
      	"monitoringConfiguration":
      	  {
      		"persistentAppUI": "ENABLED",
      		"s3MonitoringConfiguration": {
      			"logUri": "<s3-log-path>"
      		 }
      	  }
        }'
	
  Example:  
  --> aws emr-containers start-job-run \
        --virtual-cluster-id=jerdthnadi6lzionex9shyw71 \
        --name=2482-spark-on-eks-demo \
        --execution-role-arn=arn:aws:iam::076931226898:role/2482-misc-service-role \
        --release-label=emr-6.3.0-latest \
        --job-driver='{
          "sparkSubmitJobDriver": {
            "entryPoint": "s3://2482-bucket/code/sample.py",
            "sparkSubmitParameters": "--conf spark.executor.instances=1 --conf spark.executor.memory=1G --conf spark.executor.cores=1 --conf spark.driver.cores=1 --conf spark.kubernetes.container.image=076931226898.dkr.ecr.us-east-2.amazonaws.com/2482-spark-images"
          }
        }'\
        --configuration-overrides='{
        	"applicationConfiguration": [
            {
              "classification": "spark-defaults", 
              "properties": {
                "spark.dynamicAllocation.enabled":"true",
                "spark.dynamicAllocation.shuffleTracking.enabled":"true",
                "spark.dynamicAllocation.minExecutors":"1",
                "spark.dynamicAllocation.maxExecutors":"10",
                "spark.dynamicAllocation.initialExecutors":"1",
                "spark.dynamicAllocation.schedulerBacklogTimeout": "1s",
                "spark.dynamicAllocation.executorIdleTimeout": "5s"
               }
            }
          ],
      	"monitoringConfiguration":
      	  {
      		"persistentAppUI": "ENABLED",
      		"s3MonitoringConfiguration": {
      			"logUri": "s3://2482-bucket/logs/"
      		 }
      	  }
        }'
